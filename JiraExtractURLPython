import requests
import pandas as pd
from requests.auth import HTTPBasicAuth

# Your Jira details
JIRA_URL = 'https://your-domain.atlassian.net'
JQL = 'your JQL query here'
EMAIL = 'your-email@domain.com'
API_TOKEN = 'your_api_token'

auth = HTTPBasicAuth(EMAIL, API_TOKEN)
headers = {
    "Accept": "application/json"
}

# Construct the request URL
url = f"{JIRA_URL}/rest/api/3/search?jql={JQL}"

response = requests.request(
    "GET",
    url,
    headers=headers,
    auth=auth
)

# Check if the response is successful
if response.status_code == 200:
    data = response.json()
    issues = data['issues']
    
    # Assuming your JQL returns fields like summary, status, assignee, etc.
    # Adjust these fields based on what you need
    columns = ['key', 'summary', 'status', 'assignee']
    rows = []

    for issue in issues:
        # Adjust the field extraction based on your actual data structure
        key = issue['key']
        summary = issue['fields']['summary']
        status = issue['fields']['status']['name']
        assignee = issue['fields']['assignee']['displayName'] if issue['fields']['assignee'] else 'Unassigned'
        
        rows.append([key, summary, status, assignee])

    # Create a DataFrame
    df = pd.DataFrame(rows, columns=columns)
    
    # Save the DataFrame to an Excel file
    excel_file_path = 'jira_issues.xlsx'
    df.to_excel(excel_file_path, index=False)
    print(f"Data saved to {excel_file_path}")
else:
    print("Failed to fetch data:", response.text)






# korn shell script in Linux
#!/bin/bash

# Array of Documentum file URLs
urls=(
  "https://documentum.example.com/file1"
  "https://documentum.example.com/file2"
)

# Directory where files will be saved
save_dir="/mnt/shared_drive/documents"

# Ensure the save directory exists
mkdir -p "$save_dir"

# Loop through URLs and download each file
for url in "${urls[@]}"; do
  wget -P "$save_dir" "$url"
done






# Powershell .ps1 script

# Array of Documentum file URLs
$urls = @(
    "https://documentum.example.com/file1",
    "https://documentum.example.com/file2"
)

# Directory where files will be saved
$saveDir = "Z:\shared_drive\documents" # Adjust the path as needed

# Ensure the save directory exists
if (-not (Test-Path -Path $saveDir)) {
    New-Item -ItemType Directory -Path $saveDir
}

# Loop through URLs and download each file
foreach ($url in $urls) {
    $fileName = [System.IO.Path]::GetFileName($url)
    $destinationPath = Join-Path -Path $saveDir -ChildPath $fileName
    Invoke-WebRequest -Uri $url -OutFile $destinationPath
}








# Python script

import os
import requests

# List of Documentum file URLs
urls = [
    "https://documentum.example.com/file1",
    "https://documentum.example.com/file2",
]

# Directory where files will be saved
save_dir = "/mnt/shared_drive/documents"  # Adjust for Windows or Linux path

# Ensure the save directory exists
os.makedirs(save_dir, exist_ok=True)

# Download each file
for url in urls:
    response = requests.get(url)
    filename = os.path.join(save_dir, url.split('/')[-1])

    with open(filename, 'wb') as f:
        f.write(response.content)

