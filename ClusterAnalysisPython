import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import openpyxl
from openpyxl.utils.dataframe import dataframe_to_rows

# Function to preprocess text
def preprocess_text(text):
    # Tokenize and remove punctuation
    tokens = word_tokenize(text)
    tokens = [word.lower() for word in tokens if word.isalpha()]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    return ' '.join(tokens)

# Load data
df = pd.read_excel('your_file.xlsx')

# Preprocess the 'Findings' column
df['Findings'] = df['Findings'].apply(preprocess_text)

# Vectorize the findings
vectorizer = TfidfVectorizer(max_features=500)
X = vectorizer.fit_transform(df['Findings'])

# Cluster the data
n_clusters = 5  # Modify as needed
kmeans = KMeans(n_clusters=n_clusters)
clusters = kmeans.fit_predict(X)
df['Cluster'] = clusters

# Create a mapping of clusters to keywords
cluster_keywords = {}
for i in range(n_clusters):
    cluster_center = kmeans.cluster_centers_[i]
    terms = vectorizer.get_feature_names_out()
    sorted_terms = [terms[ind] for ind in cluster_center.argsort()[-10:]]
    cluster_keywords[i] = ', '.join(sorted_terms)

# Map clusters to keywords
df['Cluster_Label'] = df['Cluster'].map(cluster_keywords)

# Group and count
grouped_df = df.groupby(['DocumentType', 'Cluster_Label']).size().reset_index(name='Counts')

# Save to Excel
wb = openpyxl.Workbook()
sheet = wb.active

# Write the grouped DataFrame to the sheet
for row in dataframe_to_rows(grouped_df, index=False, header=True):
    sheet.append(row)

# Save the workbook
output_file = 'clustered_data.xlsx'
wb.save(output_file)

print(f'Workbook saved as {output_file}')
