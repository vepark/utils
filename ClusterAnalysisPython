import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from textblob import TextBlob
import openpyxl

def extract_key_phrases(text):
    blob = TextBlob(text)
    return [phrase for phrase in blob.noun_phrases]

# Read the Excel file
df = pd.read_excel('your_file.xlsx')  # Replace with your file path

# Vectorize the Findings
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['Findings'])

# Cluster the Findings
n_clusters = 5  # Adjust the number of clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
clusters = kmeans.fit_predict(X)

# Assign the cluster numbers to the DataFrame
df['Cluster'] = clusters

# Group by DocumentType and Cluster
grouped = df.groupby(['DocumentType', 'Cluster'])

# Create a new DataFrame for the output
output = pd.DataFrame(columns=['DocumentType', 'Cluster', 'KeyPhrases', 'Count'])

# Extract key phrases and count for each group
for name, group in grouped:
    all_text = " ".join(group['Findings'])
    key_phrases = extract_key_phrases(all_text)
    key_phrases_str = ', '.join(set(key_phrases))  # Convert to string
    output = output.append({'DocumentType': name[0], 'Cluster': name[1], 'KeyPhrases': key_phrases_str, 'Count': len(group)}, ignore_index=True)

# Save the output to a new Excel file
output_file = 'clustered_findings.xlsx'
output.to_excel(output_file, index=False)

print(f'Clustered findings saved to {output_file}')














import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import openpyxl
from openpyxl.utils.dataframe import dataframe_to_rows

# Function to preprocess text
def preprocess_text(text):
    # Tokenize and remove punctuation
    tokens = word_tokenize(text)
    tokens = [word.lower() for word in tokens if word.isalpha()]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    return ' '.join(tokens)

# Load data
df = pd.read_excel('your_file.xlsx')

# Preprocess the 'Findings' column
df['Findings'] = df['Findings'].apply(preprocess_text)

# Vectorize the findings
vectorizer = TfidfVectorizer(max_features=500)
X = vectorizer.fit_transform(df['Findings'])

# Cluster the data
n_clusters = 5  # Modify as needed
kmeans = KMeans(n_clusters=n_clusters)
clusters = kmeans.fit_predict(X)
df['Cluster'] = clusters

# Create a mapping of clusters to keywords
cluster_keywords = {}
for i in range(n_clusters):
    cluster_center = kmeans.cluster_centers_[i]
    terms = vectorizer.get_feature_names_out()
    sorted_terms = [terms[ind] for ind in cluster_center.argsort()[-10:]]
    cluster_keywords[i] = ', '.join(sorted_terms)

# Map clusters to keywords
df['Cluster_Label'] = df['Cluster'].map(cluster_keywords)

# Group and count
grouped_df = df.groupby(['DocumentType', 'Cluster_Label']).size().reset_index(name='Counts')

# Save to Excel
wb = openpyxl.Workbook()
sheet = wb.active

# Write the grouped DataFrame to the sheet
for row in dataframe_to_rows(grouped_df, index=False, header=True):
    sheet.append(row)

# Save the workbook
output_file = 'clustered_data.xlsx'
wb.save(output_file)

print(f'Workbook saved as {output_file}')
