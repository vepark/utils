import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk

# Ensure you have the necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

def process_text(text):
    """ Tokenize text and remove stop words. """
    tokens = word_tokenize(text)
    tokens = [word.lower() for word in tokens if word.isalpha()]
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    return ' '.join(tokens)

def cluster_findings(file_path):
    # Read the Excel file
    df = pd.read_excel(file_path)

    # Process the Findings column
    df['ProcessedFindings'] = df['Findings'].apply(process_text)

    # Vectorize the processed text
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(df['ProcessedFindings'])

    # Perform clustering
    n_clusters = 5  # You can adjust the number of clusters here
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(X)

    # Assign the cluster labels to the dataframe
    df['Cluster'] = kmeans.labels_

    # Group by DocumentType and Cluster
    grouped_df = df.groupby(['DocumentType', 'Cluster'])

    # Save the grouped results to a new Excel file
    grouped_df.size().to_excel('clustered_findings.xlsx')

# Replace 'your_file.xlsx' with the path to your Excel file
cluster_findings('your_file.xlsx')
