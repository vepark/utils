import pandas as pd
import nltk
from nltk.cluster import KMeansClusterer
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize
from collections import defaultdict

# Download the punkt tokenizer models
nltk.download('punkt')

# Function to tokenize text
def tokenize(text):
    return word_tokenize(text)

# Load your Excel file
df = pd.read_excel('your_file.xlsx')

# Ensure text is string
df['Findings'] = df['Findings'].astype(str)

# TF-IDF Vectorizer
tfidf_vect = TfidfVectorizer(tokenizer=tokenize)
tfidf_matrix = tfidf_vect.fit_transform(df['Findings'])

# Number of clusters
NUM_CLUSTERS = 5

# KMeans Clustering
kmeans = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)
clusters = kmeans.cluster(np.array(tfidf_matrix.todense()), assign_clusters=True)

# Assigning the clusters to each row
df['Cluster'] = clusters

# Group by Document Type and Cluster
grouped = df.groupby(['DocumentType', 'Cluster'])

# Aggregate the findings in each cluster for each Document Type
clustered_findings = defaultdict(list)
for name, group in grouped:
    doc_type, cluster = name
    clustered_findings[doc_type].append((cluster, ', '.join(group['Findings'])))

# Write to a new Excel file
with pd.ExcelWriter('clustered_findings.xlsx') as writer:
    for doc_type, findings in clustered_findings.items():
        temp_df = pd.DataFrame(findings, columns=['Cluster', 'Findings'])
        temp_df.to_excel(writer, sheet_name=doc_type[:31])

print("Clustering complete. Results saved to 'clustered_findings.xlsx'.")
